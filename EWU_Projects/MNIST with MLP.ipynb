{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Predicting Hand Written Digits Again\n",
    "\n",
    "CSCD439 - Machine Learning\n",
    "\n",
    "Richard Teller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load in the MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import idx2numpy\n",
    "\n",
    "X_train = idx2numpy.convert_from_file('train-images-idx3-ubyte')\n",
    "y_train = idx2numpy.convert_from_file('train-labels-idx1-ubyte')\n",
    "X_test = idx2numpy.convert_from_file('t10k-images-idx3-ubyte')\n",
    "y_test = idx2numpy.convert_from_file('t10k-labels-idx1-ubyte')\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets flatten the images into a one dimensional array of pixels so we can use it as our input layer to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, our data is ready to be used.  For this assignment we will be creating a logistic regression and a few MLP classifier models.  So let's import the libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model 1 - Logistic Regression using lbfgs solver\n",
    "\n",
    "logisticModel = LogisticRegression(solver = 'lbfgs')\n",
    "\n",
    "logisticModel.fit(X_train, y_train)\n",
    "y_predicted = logisticModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9172 \n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96       980\n",
      "          1       0.96      0.98      0.97      1135\n",
      "          2       0.93      0.88      0.90      1032\n",
      "          3       0.90      0.91      0.90      1010\n",
      "          4       0.93      0.93      0.93       982\n",
      "          5       0.90      0.85      0.87       892\n",
      "          6       0.94      0.95      0.94       958\n",
      "          7       0.93      0.92      0.92      1028\n",
      "          8       0.84      0.88      0.86       974\n",
      "          9       0.90      0.90      0.90      1009\n",
      "\n",
      "avg / total       0.92      0.92      0.92     10000\n",
      " \n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      " [[ 957    0    0    4    0    3    6    2    6    2]\n",
      " [   0 1116    3    1    0    1    4    1    8    1]\n",
      " [   8   12  906   18    9    5   10   11   50    3]\n",
      " [   3    0   19  916    2   23    5   11   24    7]\n",
      " [   1    2    5    3  910    0   11    2   10   38]\n",
      " [  11    2    1   40   10  754   16    8   39   11]\n",
      " [   7    3    7    2    4   17  909    1    8    0]\n",
      " [   3    6   24    4    7    1    1  946    5   31]\n",
      " [   9   15    7   22   11   26    7   12  854   11]\n",
      " [   9    6    2   13   30    4    0   25   16  904]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), \"\\n\")\n",
    "print(\"Classification Report:\\n\\n\", classification_report(y_test, y_predicted), \"\\n\")\n",
    "print(\"Confusion Matrix:\\n\\n\", confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 - One hidden layer MLP with 50 nodes\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50), random_state=1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_predicted = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8756 \n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.92      0.94       980\n",
      "          1       0.99      0.97      0.98      1135\n",
      "          2       0.81      0.89      0.85      1032\n",
      "          3       0.86      0.81      0.84      1010\n",
      "          4       0.93      0.78      0.85       982\n",
      "          5       0.84      0.75      0.79       892\n",
      "          6       0.93      0.93      0.93       958\n",
      "          7       0.95      0.88      0.91      1028\n",
      "          8       0.76      0.87      0.81       974\n",
      "          9       0.77      0.92      0.84      1009\n",
      "\n",
      "avg / total       0.88      0.88      0.88     10000\n",
      " \n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      " [[ 906    0    3    4    0   36    9    2   20    0]\n",
      " [   0 1097   11    6    0    2    3    1   14    1]\n",
      " [   7    0  919    9    7    0   15    7   61    7]\n",
      " [   3    0   84  821    0   48    1   11   32   10]\n",
      " [   1    0    4    1  767    4   17    2    6  180]\n",
      " [   7    1    6   81    2  669   11    3   95   17]\n",
      " [   6    2    4    1   23   14  894    0   14    0]\n",
      " [   0    6   65    4    4    1    0  906    2   40]\n",
      " [   5    1   39   17    2   19    8    7  848   28]\n",
      " [   4    6    2   11   20    4    0   14   19  929]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), \"\\n\")\n",
    "print(\"Classification Report:\\n\\n\", classification_report(y_test, y_predicted), \"\\n\")\n",
    "print(\"Confusion Matrix:\\n\\n\", confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 - One hidden layer MLP with 100 nodes\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100), random_state=1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_predicted = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9451 \n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.97       980\n",
      "          1       0.98      0.98      0.98      1135\n",
      "          2       0.94      0.95      0.94      1032\n",
      "          3       0.93      0.93      0.93      1010\n",
      "          4       0.95      0.94      0.95       982\n",
      "          5       0.93      0.92      0.93       892\n",
      "          6       0.96      0.96      0.96       958\n",
      "          7       0.96      0.93      0.95      1028\n",
      "          8       0.91      0.95      0.93       974\n",
      "          9       0.92      0.92      0.92      1009\n",
      "\n",
      "avg / total       0.95      0.95      0.95     10000\n",
      " \n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      " [[ 945    1    6    1    0    5   10    6    6    0]\n",
      " [   0 1109    3    3    0    1    2    1   15    1]\n",
      " [   4    3  977   16    5    2    6   10    6    3]\n",
      " [   1    2   13  943    0   20    1    3   19    8]\n",
      " [   0    1   10    1  923    3    6    2    6   30]\n",
      " [   5    1    0   24    1  822   13    2   18    6]\n",
      " [   7    4    5    0    3    9  922    2    6    0]\n",
      " [   2    4   20    7    5    4    0  955    5   26]\n",
      " [   5    0    6    9    5   10    4    4  922    9]\n",
      " [   3    5    4    7   29    7    0    7   14  933]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), \"\\n\")\n",
    "print(\"Classification Report:\\n\\n\", classification_report(y_test, y_predicted), \"\\n\")\n",
    "print(\"Confusion Matrix:\\n\\n\", confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 - One hidden layer MLP with 400 nodes\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(400), random_state=1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_predicted = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9678 \n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.97      0.96      0.97      1032\n",
      "          3       0.96      0.97      0.96      1010\n",
      "          4       0.97      0.97      0.97       982\n",
      "          5       0.96      0.95      0.95       892\n",
      "          6       0.97      0.97      0.97       958\n",
      "          7       0.97      0.96      0.97      1028\n",
      "          8       0.96      0.95      0.96       974\n",
      "          9       0.95      0.96      0.96      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      " \n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      " [[ 965    0    2    2    1    2    3    2    2    1]\n",
      " [   0 1120    3    2    0    2    2    3    3    0]\n",
      " [   5    3  994    6    4    0    2    7    9    2]\n",
      " [   1    0    3  980    0    9    0    3    5    9]\n",
      " [   1    0    5    0  952    1    2    4    2   15]\n",
      " [   4    0    2   16    2  848    8    1    4    7]\n",
      " [   4    2    2    1    6   11  927    1    4    0]\n",
      " [   1    3   10    3    3    0    0  991    4   13]\n",
      " [   2    1    3   14    4    7    5    3  929    6]\n",
      " [   0    4    1    2   11    5    2    8    4  972]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), \"\\n\")\n",
    "print(\"Classification Report:\\n\\n\", classification_report(y_test, y_predicted), \"\\n\")\n",
    "print(\"Confusion Matrix:\\n\\n\", confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5 - Two hidden layer MLP with 100 and 50 nodes respectively\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, 50), random_state=1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_predicted = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.966 \n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.96      0.96      0.96      1032\n",
      "          3       0.96      0.96      0.96      1010\n",
      "          4       0.97      0.96      0.96       982\n",
      "          5       0.95      0.96      0.96       892\n",
      "          6       0.98      0.97      0.97       958\n",
      "          7       0.97      0.97      0.97      1028\n",
      "          8       0.96      0.96      0.96       974\n",
      "          9       0.95      0.94      0.95      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      " \n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      " [[ 963    0    3    0    1    5    1    5    2    0]\n",
      " [   0 1122    5    1    0    1    1    0    4    1]\n",
      " [   7    1  989   13    3    2    3    7    7    0]\n",
      " [   0    3   10  972    1    9    0    3    9    3]\n",
      " [   1    0    5    0  943    1    9    1    2   20]\n",
      " [   2    0    1   12    0  859    5    0    6    7]\n",
      " [   8    2    0    0    7    6  931    0    4    0]\n",
      " [   3    3   10    1    0    0    0  993    5   13]\n",
      " [   4    1    7    5    3    8    2    5  937    2]\n",
      " [   2    3    0    9   17   11    1   11    4  951]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), \"\\n\")\n",
    "print(\"Classification Report:\\n\\n\", classification_report(y_test, y_predicted), \"\\n\")\n",
    "print(\"Confusion Matrix:\\n\\n\", confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "**Which model gives the best accuracy? Which the best overall F1 score?**\n",
    "\n",
    "From the above results we can see that the MLP model with 400 nodes has the best accuracy with a score of 96.78%.  The best F1 score is tied between the MLP model with 400 nodes and the MLP model with 100 and 50 nodes, their F1 score is 0.97.\n",
    "\n",
    "**Which model gives the worst accuracy? Which the worst overall F1 score?**\n",
    "\n",
    "The model with the worst accuracy is the MLP model with only one hidden layer of 50 nodes, its accuracy is 87.56%.  This model also has the worst F1 score of 0.88.  Even the logistic model is doing better than this neural network.\n",
    "\n",
    "**What is the shape of the training set? How many nodes are in the input layer of the network?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape is 60,000x784.  The 60,000 meaning we have 60,000 training images, and we flattened each image to be 784 pixels in one dimension.  \n",
    "\n",
    "The input of our problem is the training images, but furthermore, the pixels of those images.  The images are 28x28 pixels and when flattened is 784 total pixels.  Thus, the input layer to the neural network are those 784 pixels, so the input layer has 784 nodes, with each node representing the color value of each pixel.\n",
    "\n",
    "**Look at the documentation for MLPClassifier. Why are we using lbfgs solver? Look up l-bfgs and provide a description of what it does.**\n",
    "\n",
    "l-bfgs is our optimization function.  So instead of using Gradient Descent we are using l-bfgs.  These solvers attempt to find the minimum of the cost function, which would be where the derivative is zero.  They do this by approximating Newton's Method for finding minimums.  Part of performing this method involves calculating the Hessian matrix.  The inverse of this matrix is also used in the method and is a very expensive calculation.  The normal bfgs method approximates the inverse of this matrix.  However, the size of this Hessian matrix is nxn where n is the number of features in your data set.  When the number of features grows too big, the approximation of the Hessian matrix can become too large to store in memory, and thus l-bfgs is introduced.  l-bfgs stores only parts of the approximation of the Hessian matrix so that we can still reconstruct the matrix, but we save a lot more memory in doing so.  Our data set is 28x28 pixel images, and when flattened gives us the 784 pixel input.  Thus we have 784 features.  This would mean the Hessian matrix would be 784x784 in size which is about 620,000 total values.  This is a very large number of calculations we would have to perform and thus, it is a good idea for us in this case, with our large number of features (784), to use l-bfgs as our solver.  The l-bfgs solver is better suited for data sets with a larger number of features.\n",
    "\n",
    "**Why do you think the best/worst networks are that way?**\n",
    "\n",
    "I think the shape of the network has something to do with how well the model performs.  In all the models our input layer is 784 nodes and the output layer is 10 nodes for each digit.  So already we have a decrease in nodes for the next layer over.  It seems as though if this decrease in nodes for the next layer is gradual the network performs better.  The first MLP with one hidden layer of 50 nodes goes 784->50->10, which has a large jump from 784 nodes to 50 nodes.  This network didn't perform too well with an accuracy of 87.56%.  Jumping forward, the network with 400 hidden layer nodes goes 784->400->10, which still has some large jumps, say from 400 to 10, but 784 to 400 is much better than the previous model going from 784 to 50.  The 400 node model performed the best out of all of the models with an accuracy of 96.78%.  The model with two hidden layers of 100 and 50 nodes respectively performed just as good as the 400 node model.  This model looks like 784->100->50->10 and although it does have a big jump from 784 nodes to 100 nodes, the overall shape of the network is smoother since it has more layers.  Just for fun I tried a few extra MLP models with better network shapes including some with three hidden layers to see what would happen.  The following was the best one I tried:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra - Two hidden layer MLP with 400, 150 nodes respectively\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(400, 150), random_state=1)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_predicted = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9737 \n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98       980\n",
      "          1       0.99      0.99      0.99      1135\n",
      "          2       0.98      0.96      0.97      1032\n",
      "          3       0.96      0.98      0.97      1010\n",
      "          4       0.98      0.98      0.98       982\n",
      "          5       0.96      0.97      0.96       892\n",
      "          6       0.97      0.98      0.97       958\n",
      "          7       0.98      0.97      0.98      1028\n",
      "          8       0.97      0.97      0.97       974\n",
      "          9       0.97      0.97      0.97      1009\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      " \n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      " [[ 963    1    0    0    2    3    5    1    2    3]\n",
      " [   0 1122    0    2    3    1    2    1    4    0]\n",
      " [   2    4  994    8    1    2    7    4    9    1]\n",
      " [   0    0    5  988    0    8    0    2    3    4]\n",
      " [   0    1    6    1  959    1    3    1    0   10]\n",
      " [   3    0    2    9    0  863    8    0    5    2]\n",
      " [   5    3    1    1    4    5  937    0    2    0]\n",
      " [   0    5    7   10    0    2    0  996    2    6]\n",
      " [   1    0    3    7    2    6    3    5  940    7]\n",
      " [   3    2    0    4   12    6    2    4    1  975]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted), \"\\n\")\n",
    "print(\"Classification Report:\\n\\n\", classification_report(y_test, y_predicted), \"\\n\")\n",
    "print(\"Confusion Matrix:\\n\\n\", confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment and try to create a better performing network using tensorflow. Explain what you tried and document the results.**\n",
    "\n",
    "TensorFlow implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# reload the data into a single object\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"./\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.test.cls = np.array([label.argmax() for label in data.test.labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our graph\n",
    "\n",
    "# Variables:\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "y_true_cls = tf.placeholder(tf.int64, [None])\n",
    "weights = tf.Variable(tf.zeros([784, 10]))\n",
    "biases = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Logits function:\n",
    "logits = tf.matmul(x, weights) + biases\n",
    "y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "\n",
    "# Cost function:\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Optimizer:\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=.8).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize the session\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(num_iterations):\n",
    "    for i in range(num_iterations):\n",
    "        # split the 60000 images into batches of 100\n",
    "        x_batch, y_true_batch = data.train.next_batch(100)\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "feed_dict_test = {x: data.test.images,\n",
    "                  y_true: data.test.labels,\n",
    "                  y_true_cls: data.test.cls}\n",
    "\n",
    "def print_accuracy():\n",
    "    print(\"Accuracy: \", session.run(accuracy, feed_dict=feed_dict_test), \"\\n\")\n",
    "    \n",
    "def print_confusion_matrix():\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = data.test.cls\n",
    "    \n",
    "    # Get the predicted classifications for the test-set.\n",
    "    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\\n\", confusion_matrix(y_true=cls_true, y_pred=cls_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9221 \n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      " [[ 954    0    1    2    1    9    5    5    3    0]\n",
      " [   0 1103    6    0    1    2    3    2   18    0]\n",
      " [   3    6  932    8   14    3   10    7   45    4]\n",
      " [   4    1   23  898    3   24    1   11   34   11]\n",
      " [   1    1    7    2  933    0    5    2    7   24]\n",
      " [  10    2    2   30   14  769   10    7   40    8]\n",
      " [   7    3    7    2   15   24  893    2    5    0]\n",
      " [   1    7   27    6   12    2    0  924    1   48]\n",
      " [   5    6    7   13    9   19    8    5  890   12]\n",
      " [   6    6    2    6   33    6    0   13   12  925]]\n"
     ]
    }
   ],
   "source": [
    "# run 6000 iterations\n",
    "optimize(6000)\n",
    "\n",
    "print_accuracy()\n",
    "print_confusion_matrix()\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the TensorFlow implementation, I defined all of the pieces to the graph.  I used logits, gradient descent, cross entropy, with softmax on the output layer.\n",
    "\n",
    "I experimented by changing the hyper-parameters.  I tried learning rate values between 0.1 up to 1.5, as well as changing the batch size between values of 60 to 600 and the number of optimization iterations between 600 to 10000.\n",
    "\n",
    "Some examples of what I tried:\n",
    "\n",
    "- learning rate=.5, batch size=60, iterations=1000, Accuracy recieved was: 91.19%\n",
    "- learning rate=1.5, batch size=100, iterations=5000, Accuracy recieved was: 92.17% \n",
    "- learning rate=.8, batch size=60, iterations=10000, Accuracy recieved was: 92.01%\n",
    "\n",
    "The best setup I tested used learning rate=.8, batch size=100, iterations=6000.  This model gave an accuracy of 92.21% and is depicted above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
